# 一次抽四支

<div align="center">
  <img src="images/avatar.png " alt="Profile Avatar" width="200"/>
</div>

## 学习笔记：

1. [强化学习（Reinforcement Learning）学习笔记](https://cheng-1018.github.io/RL-)

| 章节 | 主题 | 内容 |
|------|------|------|
| 1 | **马尔可夫决策过程** | 马尔可夫性质、马尔可夫奖励过程、马尔可夫决策过程、价值函数与贝尔曼方程、策略迭代与价值迭代 |
| 2 | **表格型方法** | 蒙特卡洛方法、时序差分学习、Q-Learning算法、Sarsa算法、免模型预测与控制 |
| 3 | **策略梯度** | 策略梯度定理、REINFORCE算法 |
| 4 | **PPO算法** | PPO-Clip、PPO-Penalty、TRPO |
| 5 | **DQN介绍** | 状态价值函数、动作价值函数、目标网络、探索、经验回放 |
| 6 | **DQN进阶技巧** | DDQN、dueling DQN、PER、TD(N)、噪声网络、分布式Q函数、彩虹 |
| 7 | **连续动作DQN** | 对动作采样、梯度上升、设计网络架构 |
| 8 | **演员-评论员** | 优势演员-评论员、异步演员-评论员、路径衍生策略梯度 |
| 9 | **稀疏奖励与模仿学习** | 课程学习、逆向课程学习、分层强化学习、行为克隆、逆强化学习 |
| 10 | **DDPG** | DDPG、Twin Delayed DDPG |

2. [AI原理学习笔记](https://cheng-1018.github.io/AIPrincipleNote)

| 章节 | 主题 | 内容 |
|------|------|------|
| 1 | **Transformer** | Embedding、位置编码、注意力机制、FNN、LayerNorm、训练预测 |
| 2 | **预训练语言模型** | Encoder Only:BERT,RoBERTa,ALBERT；Encoder-Decoder:T5；Decoder:GPT,LLaMA,GLM |
| 3 | **大模型训练** | Pretrain、SFT、RLHF:PPO,DPO |
| 4 | **分词算法** | BPE、WordPiece、Unigram |
| 5 | **网络训练优化** | SGD、AdaM、BN,LN归一化、参数初始化、正则化 |
| 6 | **其他架构** | MOE、ROPE、MLA |
| 7 | **解码策略** | 贪心搜索、波束搜索、TopK 采样 |
| 8 | **模型压缩** | 数据类型、量化方法、知识蒸馏、低秩分解 |

3. [AIAgent学习笔记](https://cheng-1018.github.io/AIAgent/)

| 章节 | 主题 | 内容 |
|------|------|------|
| 1 | **经典智能体范式** | ReAct、Plan and Solve、Reflect |
| 2 | **框架介绍** | AutoGen、AgentScope、CAMEL、LangGraph |
| 3 | **框架开发实践** | Agent Tool Memory RAG protocol |
| 4 | **智能体评估** | BenchMark BFCL GAIA |
| 5 | **智能体实践** | //TODO  |


4. [machinelearning](https://cheng-1018.github.io/machinelearning/),感兴趣有时间就写。

| 章节 | 主题 | 内容 |
|------|------|------|
| 1 | **数学基础补充** | 拉格朗日乘子法、多元高斯分布 |
| 2 | **感兴趣的补充** | EM算法 |
| 3 | **SVM** | 基本型、对偶问题、核函数、软间隔与SVR |
| 4 | **贝叶斯** | 贝叶斯决策论、极大似然估计、朴素贝叶斯 |
   
5. [前端learn](https://cheng-1018.github.io/architectural/)



